# GPT Researcher 日志与回调文档

## 1. 日志系统实现

### 1.1 日志框架

GPT Researcher 使用 Python 原生 `logging` 模块作为核心日志框架，配合自定义扩展实现功能丰富的日志系统：

#### 主要组件
- **Python logging**: 核心日志框架
- **自定义格式化器**: 带颜色的控制台输出
- **JSON 处理器**: 结构化研究日志
- **WebSocket 流式输出**: 实时日志传输

### 1.2 日志级别和分类

#### 日志级别定义
```python
# 文件: utils/logger.py
TRACE_LOG_LEVEL = 5  # 自定义跟踪级别
logging.DEBUG        # 调试级别
logging.INFO         # 信息级别（默认）
logging.WARNING      # 警告级别
logging.ERROR        # 错误级别
```

#### 日志分类
```python
# 主要日志记录器
logger = logging.getLogger('research')      # 研究操作日志
scraper_logger = logging.getLogger('scraper')  # 抓取操作日志
module_logger = logging.getLogger(__name__)  # 模块特定日志
```

### 1.3 日志格式化

#### 控制台格式化（带颜色）
```python
# 文件: utils/logger.py
formatter = DefaultFormatter(
    "%(levelprefix)s [%(asctime)s] %(message)s",
    datefmt="%H:%M:%S"
)

# 级别颜色定义
level_name_colors = {
    logging.DEBUG: lambda level_name: click.style(str(level_name), fg="cyan"),
    logging.INFO: lambda level_name: click.style(str(level_name), fg="green"),
    logging.WARNING: lambda level_name: click.style(str(level_name), fg="yellow"),
    logging.ERROR: lambda level_name: click.style(str(level_name), fg="red"),
}
```

#### JSON 结构化日志
```python
# 文件: utils/logging_config.py
class JsonLogFormatter(logging.Formatter):
    """JSON 格式化器，用于结构化日志"""
    def format(self, record):
        log_entry = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if hasattr(record, 'research_data'):
            log_entry['research_data'] = record.research_data
        return json.dumps(log_entry)
```

### 1.4 日志输出目标

#### 多目标输出
1. **控制台输出**: 实时彩色日志
2. **文件日志**: 带时间戳的日志文件
3. **JSON 日志**: 结构化研究事件记录
4. **WebSocket 流**: 实时传输到前端

#### 日志文件配置
```python
# 文件: utils/logging_config.py
def setup_research_logging(log_dir="logs"):
    """设置研究日志系统"""
    # 创建日志目录
    os.makedirs(log_dir, exist_ok=True)
    
    # 文件处理器
    file_handler = logging.FileHandler(
        f"{log_dir}/research_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )
    
    # JSON 处理器
    json_handler = logging.FileHandler(
        f"{log_dir}/research_events_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    json_handler.setFormatter(JsonLogFormatter())
```

### 1.5 性能日志

#### Token 和成本跟踪
```python
# 文件: actions/utils.py
async def update_cost(
    prompt_tokens: int,
    completion_tokens: int,
    model: str,
    websocket: Any
) -> None:
    """更新成本信息并通过 WebSocket 发送"""
    cost = calculate_cost(prompt_tokens, completion_tokens, model)
    total_tokens = prompt_tokens + completion_tokens
    
    await safe_send_json(websocket, {
        "type": "cost",
        "data": {
            "total_tokens": format_token_count(total_tokens),
            "total_cost": f"${cost:.4f}"
        }
    })
```

#### 执行时间监控
```python
# 文件: skills/researcher.py
async def conduct_research(self):
    """执行研究并记录性能指标"""
    start_time = time.time()
    
    # 执行研究...
    context = await self._execute_research()
    
    # 记录执行时间
    execution_time = time.time() - start_time
    logger.info(f"Research completed in {execution_time:.2f} seconds")
    
    return context
```

### 1.6 调试日志功能

#### 详细模式控制
```python
# 文件: config/config.py
def set_verbose(self, verbose: bool) -> None:
    """设置详细级别"""
    self.llm_kwargs["verbose"] = verbose
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.INFO)
```

#### 自定义跟踪级别
```python
# 文件: utils/logger.py
def add_trace_level():
    """添加自定义跟踪级别"""
    logging.addLevelName(TRACE_LOG_LEVEL, "TRACE")
    
    def trace(self, message, *args, **kwargs):
        if self.isEnabledFor(TRACE_LOG_LEVEL):
            self._log(TRACE_LOG_LEVEL, message, args, **kwargs)
    
    logging.Logger.trace = trace
```

## 2. 回调/事件系统

### 2.1 进度回调

#### 事件类型定义
```python
# 文件: agent.py
async def _log_event(self, event_type: str, **kwargs):
    """处理日志事件的辅助方法"""
    if self.log_handler:
        match event_type:
            case "tool":
                await self.log_handler.on_tool_start(
                    kwargs.get('tool_name', ''), 
                    **kwargs
                )
            case "action":
                await self.log_handler.on_agent_action(
                    kwargs.get('action', ''), 
                    **kwargs
                )
            case "research":
                await self.log_handler.on_research_step(
                    kwargs.get('step', ''), 
                    kwargs.get('details', {})
                )
```

#### 研究步骤回调
```python
# 文件: skills/researcher.py
async def _process_research_step(self, step_name: str, data: dict):
    """处理研究步骤并触发回调"""
    # 记录步骤开始
    await self._log_event("research", step=step_name, status="started")
    
    # 执行步骤...
    result = await self._execute_step(step_name, data)
    
    # 记录步骤完成
    await self._log_event("research", step=step_name, status="completed", result=result)
    
    return result
```

### 2.2 成本跟踪回调

#### 成本回调创建
```python
# 文件: actions/utils.py
def create_cost_callback(websocket: Any) -> Callable:
    """创建成本跟踪回调函数"""
    async def cost_callback(
        prompt_tokens: int,
        completion_tokens: int,
        model: str
    ) -> None:
        await update_cost(prompt_tokens, completion_tokens, model, websocket)
    return cost_callback
```

#### 成本计算和累积
```python
# 文件: agent.py
def add_costs(self, cost: float) -> None:
    """添加研究成本"""
    self.total_costs += cost
    logger.info(f"Accumulated costs: ${self.total_costs:.4f}")

def get_costs(self) -> float:
    """获取总成本"""
    return self.total_costs
```

### 2.3 WebSocket 实时回调

#### WebSocket 管理器
```python
# 文件: backend/server/websocket_manager.py
class WebSocketManager:
    async def start_streaming(self, task, report_type, report_source, 
                            source_urls, document_urls, tone, websocket, 
                            headers=None, query_domains=[], mcp_enabled=False):
        """开始流式输出"""
        try:
            # 发送开始消息
            await websocket.send_json({
                "type": "start",
                "data": {"task": task}
            })
            
            # 创建带回调的研究器
            researcher = GPTResearcher(
                query=task,
                report_type=report_type,
                report_source=report_source,
                tone=tone,
                websocket=websocket,
                headers=headers,
                query_domains=query_domains
            )
            
            # 执行研究（会触发实时回调）
            await researcher.conduct_research()
            report = await researcher.write_report()
            
            # 发送完成消息
            await websocket.send_json({
                "type": "report",
                "data": {"report": report}
            })
            
        except WebSocketDisconnect:
            logger.info("WebSocket disconnected")
        except Exception as e:
            await websocket.send_json({
                "type": "error",
                "data": {"message": str(e)}
            })
```

#### 安全的 JSON 发送
```python
# 文件: backend/server/websocket_manager.py
async def safe_send_json(websocket, data):
    """安全发送 JSON 数据"""
    if websocket and not websocket.client_state.disconnected:
        try:
            await websocket.send_json(data)
        except Exception as e:
            logger.error(f"Error sending WebSocket message: {e}")
```

### 2.4 事件驱动架构

#### 事件流处理
```python
# 文件: mcp/streaming.py
class MCPStreamer:
    async def stream_stage_start(self, stage: str, description: str):
        """流式传输研究阶段开始"""
        await self.stream_log(f"🔧 {stage}: {description}")
    
    async def stream_stage_complete(self, stage: str, result_count: int = None):
        """流式传输研究阶段完成"""
        msg = f"✅ {stage} completed"
        if result_count:
            msg += f": {result_count} results"
        await self.stream_log(msg)
    
    async def stream_progress(self, current: int, total: int, message: str):
        """流式传输进度更新"""
        percentage = (current / total) * 100
        await self.stream_log(f"📊 {message}: {current}/{total} ({percentage:.1f}%)")
```

#### 事件监听器模式
```python
# 文件: agent.py
class LogHandler:
    """日志处理器接口"""
    async def on_tool_start(self, tool_name: str, **kwargs):
        """工具开始事件"""
        pass
    
    async def on_agent_action(self, action: str, **kwargs):
        """代理动作事件"""
        pass
    
    async def on_research_step(self, step: str, details: dict):
        """研究步骤事件"""
        pass
```

### 2.5 可扩展的 Hook 机制

#### 配置 Hook
```python
# 文件: config/config.py
class Config:
    def __init__(self, config_path: str | None = None):
        # 初始化 Hook
        self.hooks = {
            'before_research': [],
            'after_research': [],
            'on_error': [],
            'on_cost_update': []
        }
    
    def add_hook(self, event: str, callback: Callable):
        """添加事件 Hook"""
        if event in self.hooks:
            self.hooks[event].append(callback)
    
    async def trigger_hooks(self, event: str, *args, **kwargs):
        """触发事件 Hook"""
        for callback in self.hooks.get(event, []):
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(*args, **kwargs)
                else:
                    callback(*args, **kwargs)
            except Exception as e:
                logger.error(f"Error in hook {event}: {e}")
```

#### LLM 参数 Hook
```python
# 文件: agent.py
def __init__(self, **kwargs):
    # LLM 参数 Hook
    self.llm_kwargs = kwargs.get('llm_kwargs', {})
    if cfg.verbose:
        self.llm_kwargs["verbose"] = True
```

## 3. 错误处理和报告

### 3.1 异常日志

#### 结构化异常记录
```python
# 文件: actions/web_scraping.py
try:
    scraper = Scraper(urls, user_agent, cfg.scraper, worker_pool=worker_pool)
    scraped_data = await scraper.run()
    for item in scraped_data:
        if 'image_urls' in item:
            images.extend(item['image_urls'])
except Exception as e:
    # 记录详细错误信息
    logger.error({
        "error": str(e),
        "type": type(e).__name__,
        "module": "web_scraping",
        "urls_count": len(urls),
        "traceback": traceback.format_exc()
    })
    # 发送到 WebSocket
    await websocket.send_json({
        "type": "error",
        "data": {
            "message": f"Scraping error: {e}",
            "details": {
                "urls_processed": len(scraped_data) if 'scraped_data' in locals() else 0
            }
        }
    })
```

### 3.2 错误恢复机制

#### WebSocket 错误处理
```python
# 文件: backend/server/server_utils.py
async def handle_websocket_communication(websocket, manager):
    """处理 WebSocket 通信和错误恢复"""
    def run_long_running_task(awaitable: Awaitable) -> asyncio.Task:
        async def safe_run():
            try:
                await awaitable
            except asyncio.CancelledError:
                logger.info("Task cancelled.")
                raise
            except Exception as e:
                logger.error(f"Error running task: {e}\n{traceback.format_exc()}")
                # 发送错误消息到客户端
                await websocket.send_json({
                    "type": "logs",
                    "content": "error",
                    "output": f"Error: {e}",
                })
                # 触发错误 Hook
                await cfg.trigger_hooks('on_error', e, websocket)
        return asyncio.create_task(safe_run())
```

#### 重试机制
```python
# 文件: retrievers/tavily/search.py
async def search_with_retry(self, query: str, max_retries: int = 3):
    """带重试的搜索"""
    for attempt in range(max_retries):
        try:
            return await self._search(query)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            logger.warning(f"Search attempt {attempt + 1} failed, retrying...")
            await asyncio.sleep(2 ** attempt)  # 指数退避
```

### 3.3 用户友好的错误消息

#### 分级错误显示
```python
# 文件: mcp/streaming.py
async def stream_error(self, error_msg: str):
    """流式传输错误消息"""
    await self.stream_log(f"❌ {error_msg}")

async def stream_warning(self, warning_msg: str):
    """流式传输警告消息"""
    await self.stream_log(f"⚠️ {warning_msg}")

async def stream_success(self, success_msg: str):
    """流式传输成功消息"""
    await self.stream_log(f"✅ {success_msg}")
```

#### 错误类型映射
```python
# 文件: backend/server/server.py
ERROR_MESSAGES = {
    "invalid_api_key": "请检查您的 API 密钥配置",
    "rate_limit": "请求过于频繁，请稍后重试",
    "network_error": "网络连接错误，请检查网络设置",
    "timeout": "请求超时，请重试",
    "unknown": "发生未知错误，请联系管理员"
}

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """全局异常处理器"""
    error_type = type(exc).__name__
    user_message = ERROR_MESSAGES.get(error_type, ERROR_MESSAGES["unknown"])
    
    return JSONResponse(
        status_code=500,
        content={
            "error": user_message,
            "type": error_type,
            "timestamp": datetime.now().isoformat()
        }
    )
```

## 4. 系统管理特性

### 4.1 日志轮转

#### 自动日志清理
```python
# 文件: utils/logging_config.py
def setup_log_rotation(log_dir="logs", max_files=10):
    """设置日志轮转"""
    # 清理旧日志文件
    log_files = sorted(glob.glob(f"{log_dir}/*.log"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            os.remove(old_file)
```

### 4.2 性能监控

#### 关键指标收集
```python
# 文件: utils/metrics.py
class ResearchMetrics:
    def __init__(self):
        self.metrics = {
            'total_research_time': 0,
            'total_tokens_used': 0,
            'total_cost': 0,
            'queries_count': 0,
            'sources_count': 0,
            'errors_count': 0
        }
    
    def record_research(self, duration: float, tokens: int, cost: float):
        """记录研究指标"""
        self.metrics['total_research_time'] += duration
        self.metrics['total_tokens_used'] += tokens
        self.metrics['total_cost'] += cost
        self.metrics['queries_count'] += 1
    
    def get_summary(self):
        """获取指标摘要"""
        return {
            **self.metrics,
            'avg_time_per_query': self.metrics['total_research_time'] / max(self.metrics['queries_count'], 1),
            'avg_cost_per_query': self.metrics['total_cost'] / max(self.metrics['queries_count'], 1)
        }
```

### 4.3 审计日志

#### 操作审计
```python
# 文件: utils/audit.py
class AuditLogger:
    def __init__(self):
        self.audit_logger = logging.getLogger('audit')
    
    def log_api_call(self, endpoint: str, method: str, user_id: str = None):
        """记录 API 调用"""
        self.audit_logger.info({
            "event": "api_call",
            "endpoint": endpoint,
            "method": method,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat()
        })
    
    def log_research_request(self, query: str, report_type: str, user_id: str = None):
        """记录研究请求"""
        self.audit_logger.info({
            "event": "research_request",
            "query": query,
            "report_type": report_type,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat()
        })
```

## 5. 最佳实践

### 5.1 日志配置建议
```python
# 生产环境配置
LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "standard": {"format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"},
        "json": {"()": "utils.logging_config.JsonLogFormatter"}
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "standard",
            "stream": "ext://sys.stdout"
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "formatter": "json",
            "filename": "logs/research.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5
        }
    },
    "loggers": {
        "": {"level": "INFO", "handlers": ["console", "file"]},
        "research": {"level": "DEBUG", "propagate": False}
    }
}
```

### 5.2 回调使用模式
```python
# 自定义回调处理器
class CustomLogHandler:
    async def on_research_step(self, step: str, details: dict):
        # 发送到监控系统
        await send_to_monitoring_system(step, details)
        
        # 记录到数据库
        await log_to_database(step, details)
        
        # 发送通知
        if step == "completed":
            await send_completion_notification(details)

# 使用自定义处理器
researcher = GPTResearcher(
    query="AI research",
    log_handler=CustomLogHandler()
)
```

GPT Researcher 的日志和回调系统设计完善，提供了全面的监控、调试和用户反馈能力，支持实时更新、结构化日志记录和灵活的扩展机制。